{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punkt:\n",
    "This dataset is used for tokenization—the process of splitting text into individual words or sentences. The 'punkt' tokenizer is a pre-trained model that recognizes common patterns in text, such as punctuation marks and sentence boundaries. It helps the tokenizer understand where words or sentences begin and end.\n",
    "\n",
    "Pwordnet:\n",
    "This dataset provides access to WordNet, a large lexical database of English. WordNet groups English words into sets of synonyms (synsets), which helps in tasks like lemmatization. The WordNet lemmatizer uses this dataset to reduce words to their base or root form (lemmas). For instance, it can convert \"running\" to \"run\" and \"better\" to \"good.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_csv(name):\n",
    "    return pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dataset_1.csv', 'dataset_3.csv', 'dataset_4.csv', 'dataset_5.csv', 'dataset_6.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0                       You Can Smell Hillary’s Fear   \n",
      "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        Kerry to go to Paris in gesture of sympathy   \n",
      "3  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "                                               title  \\\n",
      "0  Palestinians switch off Christmas lights in Be...   \n",
      "1  China says Trump call with Taiwan president wo...   \n",
      "2   FAIL! The Trump Organization’s Credit Score W...   \n",
      "3  Zimbabwe military chief's China trip was norma...   \n",
      "4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
      "\n",
      "                                                text  label  \n",
      "0  Palestinians switched off Christmas lights at ...      1  \n",
      "1  U.S. President-elect Donald Trump’s call with ...      1  \n",
      "2  While the controversy over Trump s personal ta...      0  \n",
      "3  A trip to Beijing last week by Zimbabwe s mili...      1  \n",
      "4  There has never been a more UNCOURAGEOUS perso...      0  \n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text label  \n",
      "0  Donald Trump just couldn t wish all Americans ...  FAKE  \n",
      "1  House Intelligence Committee Chairman Devin Nu...  FAKE  \n",
      "2  On Friday, it was revealed that former Milwauk...  FAKE  \n",
      "3  On Christmas day, Donald Trump announced that ...  FAKE  \n",
      "4  Pope Francis used his annual Christmas Day mes...  FAKE  \n",
      "                                               title  \\\n",
      "0         Four ways Bob Corker skewered Donald Trump   \n",
      "1  Linklater's war veteran comedy speaks to moder...   \n",
      "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
      "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
      "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
      "\n",
      "                                                text label  \n",
      "0  Image copyright Getty Images\\nOn Sunday mornin...  REAL  \n",
      "1  “Last Flag Flying”, a comedy-drama about Vietn...  REAL  \n",
      "2  The feud broke into public view last week when...  REAL  \n",
      "3  Egypt’s Cheiron Holdings Limited won the right...  REAL  \n",
      "4  Country singer Jason Aldean, who was performin...  REAL  \n",
      "                                               title  \\\n",
      "0  Syria attack symptoms consistent with nerve ag...   \n",
      "1  Homs governor says U.S. attack caused deaths b...   \n",
      "2    Death toll from Aleppo bomb attack at least 112   \n",
      "3        Aleppo bomb blast kills six Syrian state TV   \n",
      "4  29 Syria Rebels Dead in Fighting for Key Alepp...   \n",
      "\n",
      "                                                text label  \n",
      "0  Syria attack symptoms consistent with nerve ag...  FAKE  \n",
      "1  at 0914 Homs governor says U.S. attack caused ...  FAKE  \n",
      "2  Death toll from Aleppo bomb attack at least 11...  FAKE  \n",
      "3  Aleppo bomb blast kills six Syrian state TV. A...  FAKE  \n",
      "4  29 Syria Rebels Dead in Fighting for Key Alepp...  FAKE  \n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    leer_csv(set)\n",
    "    print(leer_csv(set).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 3)\n",
      "(24353, 3)\n",
      "(44898, 3)\n",
      "(3522, 3)\n",
      "(804, 3)\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    print(leer_csv(set).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    print(leer_csv(set).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in dataset_1.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_3.csv:\n",
      "[1 0]\n",
      "Unique values in dataset_4.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_5.csv:\n",
      "['REAL' 'FAKE']\n",
      "Unique values in dataset_6.csv:\n",
      "['FAKE' 'REAL']\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Unique values in {set}:\")\n",
    "    print(df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_3 = leer_csv('dataset_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kathe\\AppData\\Local\\Temp\\ipykernel_12168\\1421717575.py:8: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'REAL' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  df_dataset_3.at[idx, 'label'] = 'REAL'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Palestinians switch off Christmas lights in Be...</td>\n",
       "      <td>Palestinians switched off Christmas lights at ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China says Trump call with Taiwan president wo...</td>\n",
       "      <td>U.S. President-elect Donald Trump’s call with ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FAIL! The Trump Organization’s Credit Score W...</td>\n",
       "      <td>While the controversy over Trump s personal ta...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zimbabwe military chief's China trip was norma...</td>\n",
       "      <td>A trip to Beijing last week by Zimbabwe s mili...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...</td>\n",
       "      <td>There has never been a more UNCOURAGEOUS perso...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Palestinians switch off Christmas lights in Be...   \n",
       "1  China says Trump call with Taiwan president wo...   \n",
       "2   FAIL! The Trump Organization’s Credit Score W...   \n",
       "3  Zimbabwe military chief's China trip was norma...   \n",
       "4  THE MOST UNCOURAGEOUS PRESIDENT EVER Receives ...   \n",
       "\n",
       "                                                text label  \n",
       "0  Palestinians switched off Christmas lights at ...  REAL  \n",
       "1  U.S. President-elect Donald Trump’s call with ...  REAL  \n",
       "2  While the controversy over Trump s personal ta...  FAKE  \n",
       "3  A trip to Beijing last week by Zimbabwe s mili...  REAL  \n",
       "4  There has never been a more UNCOURAGEOUS perso...  FAKE  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dataset 3: These articles are classified as true (1) or false (0)\n",
    "\n",
    "\n",
    "for idx, label in enumerate(df_dataset_3['label']): \n",
    "    if label == 0:\n",
    "        df_dataset_3.at[idx, 'label'] = 'FAKE'\n",
    "    elif label == 1:\n",
    "        df_dataset_3.at[idx, 'label'] = 'REAL'\n",
    "\n",
    "df_dataset_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_3.to_csv('dataset_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in dataset_1.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_3.csv:\n",
      "['REAL' 'FAKE']\n",
      "Unique values in dataset_4.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_5.csv:\n",
      "['REAL' 'FAKE']\n",
      "Unique values in dataset_6.csv:\n",
      "['FAKE' 'REAL']\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Unique values in {set}:\")\n",
    "    print(df['label'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in dataset_1.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_3.csv:\n",
      "title    0\n",
      "text     1\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_4.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_5.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_6.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Null values in {set}:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_5 = leer_csv('dataset_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_5= df_dataset_5.dropna()\n",
    "df_dataset_5.to_csv('dataset_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in dataset_1.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_3.csv:\n",
      "title    0\n",
      "text     1\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_4.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_5.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_6.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Null values in {set}:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79912, 3)\n"
     ]
    }
   ],
   "source": [
    "df_list = []  # Initialize an empty list to store DataFrames\n",
    "\n",
    "for dataset in datasets:  # Loop through each file name\n",
    "    df_list.append(leer_csv(dataset))  # Append the DataFrame to the list\n",
    "\n",
    "df_combined = pd.concat(df_list, ignore_index=True)  # Merge all DataFrames\n",
    "\n",
    "print(df_combined.shape)  # Check the size of the final dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `df` is your merged dataset\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save for later use\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (63929, 3), test shape: (15983, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'train shape: {train_df.shape}, test shape: {test_df.shape}')  # Check the size of the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "#text = \"The cats are running fast.\"\n",
    "#tokens = word_tokenize(text)\n",
    "#print(\"Tokens:\", tokens)\n",
    "\n",
    "# Lemmatize the tokens\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "#print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
