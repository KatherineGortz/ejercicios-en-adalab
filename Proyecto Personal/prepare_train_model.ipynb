{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')  # Ensure it's downloaded\n",
    "\n",
    "import re\n",
    "\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #to vectorize the tokenized words\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression #model chosen for training\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Kathe/nltk_data', 'c:\\\\Users\\\\Kathe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\nltk_data', 'c:\\\\Users\\\\Kathe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\share\\\\nltk_data', 'c:\\\\Users\\\\Kathe\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Kathe\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk #repeat import becuase it gives error sometimes\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Check where nltk is looking for data\n",
    "print(nltk.data.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"C:/Users/Kathe/nltk_data\")  # Change to your actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'each', 'between', 'am', 'are', 'too', 'this', 'nor', 'his', 'from', \"i'll\", 'having', 'who', 'you', 'against', 'here', 'being', \"doesn't\", 'most', 'off', \"i'm\", 'by', 'which', \"you're\", 'will', \"she'd\", \"hasn't\", 'why', 'up', 'hadn', 'your', \"couldn't\", 'theirs', 'whom', 'that', 'the', 'own', \"i've\", 'has', 've', 'how', 'll', 'few', 'hasn', 're', 'does', 'such', \"they're\", \"weren't\", \"isn't\", 't', 'no', 'needn', 'during', 'him', 'd', 'some', 'because', \"it's\", 'they', 'when', 'her', \"won't\", 'those', \"he's\", \"he'd\", \"you'll\", 'what', 'under', 'ourselves', 'doing', 'any', 'more', \"we'll\", 'all', 'hers', 'or', 'a', 'but', \"they'd\", 'in', 'until', 'only', 'can', \"should've\", 'to', \"haven't\", 'himself', 'did', 's', \"shouldn't\", \"don't\", 'aren', 'mightn', \"they've\", 'we', 'into', 'after', 'below', \"you'd\", 'them', \"she's\", 'where', 'doesn', 'been', 'me', 'wasn', 'my', 'isn', 'itself', 'had', \"mightn't\", 'so', 'he', \"mustn't\", \"it'll\", \"he'll\", 'won', 'just', 'ain', \"they'll\", \"that'll\", 'same', 'their', 'our', 'yourself', 'about', 'further', 'themselves', 'be', 'ma', \"we're\", 'didn', 'over', 'these', 'have', \"it'd\", 'an', 'shouldn', 'down', \"needn't\", \"aren't\", \"you've\", 'than', 'haven', 'couldn', \"we'd\", 'should', \"we've\", 'ours', 'out', 'on', 'once', 'then', 'for', 'herself', 'with', 'y', 'above', 'm', 'she', 'shan', 'i', 'were', 'again', 'do', \"hadn't\", 'of', 'other', \"she'll\", 'weren', 'mustn', 'as', 'very', 'myself', 'if', 'it', 'don', 'its', 'and', 'both', 'wouldn', 'yours', 'is', 'not', 'o', 'was', \"wouldn't\", 'at', \"i'd\", 'while', \"didn't\", \"wasn't\", 'now', 'there', \"shan't\", 'before', 'through', 'yourselves'}\n"
     ]
    }
   ],
   "source": [
    "stops = set(stopwords.words('english')) #create the list of stopwords to remove from title and body text\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "punkt:\n",
    "This dataset is used for tokenization—the process of splitting text into individual words or sentences. The 'punkt' tokenizer is a pre-trained model that recognizes common patterns in text, such as punctuation marks and sentence boundaries. It helps the tokenizer understand where words or sentences begin and end.\n",
    "\n",
    "Pwordnet:\n",
    "This dataset provides access to WordNet, a large lexical database of English. WordNet groups English words into sets of synonyms (synsets), which helps in tasks like lemmatization. The WordNet lemmatizer uses this dataset to reduce words to their base or root form, like converting \"running\" to \"run\" and \"better\" to \"good.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leer_csv(name): #function to read csv\n",
    "    return pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dataset_1.csv', 'dataset_3.csv', 'dataset_4.csv', 'dataset_5.csv', 'dataset_6.csv'] #list of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0                       You Can Smell Hillary’s Fear   \n",
      "1  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        Kerry to go to Paris in gesture of sympathy   \n",
      "3  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "                                               title  \\\n",
      "0         Four ways Bob Corker skewered Donald Trump   \n",
      "1  Linklater's war veteran comedy speaks to moder...   \n",
      "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
      "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
      "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
      "\n",
      "                                                text label  \n",
      "0  Image copyright Getty Images\\nOn Sunday mornin...  REAL  \n",
      "1  “Last Flag Flying”, a comedy-drama about Vietn...  REAL  \n",
      "2  The feud broke into public view last week when...  REAL  \n",
      "3  Egypt’s Cheiron Holdings Limited won the right...  REAL  \n",
      "4  Country singer Jason Aldean, who was performin...  REAL  \n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text label  \n",
      "0  Donald Trump just couldn t wish all Americans ...  FAKE  \n",
      "1  House Intelligence Committee Chairman Devin Nu...  FAKE  \n",
      "2  On Friday, it was revealed that former Milwauk...  FAKE  \n",
      "3  On Christmas day, Donald Trump announced that ...  FAKE  \n",
      "4  Pope Francis used his annual Christmas Day mes...  FAKE  \n",
      "                                               title  \\\n",
      "0         Four ways Bob Corker skewered Donald Trump   \n",
      "1  Linklater's war veteran comedy speaks to moder...   \n",
      "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
      "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
      "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
      "\n",
      "                                                text label  \n",
      "0  Image copyright Getty Images\\nOn Sunday mornin...  REAL  \n",
      "1  “Last Flag Flying”, a comedy-drama about Vietn...  REAL  \n",
      "2  The feud broke into public view last week when...  REAL  \n",
      "3  Egypt’s Cheiron Holdings Limited won the right...  REAL  \n",
      "4  Country singer Jason Aldean, who was performin...  REAL  \n",
      "                                               title  \\\n",
      "0  Syria attack symptoms consistent with nerve ag...   \n",
      "1  Homs governor says U.S. attack caused deaths b...   \n",
      "2    Death toll from Aleppo bomb attack at least 112   \n",
      "3        Aleppo bomb blast kills six Syrian state TV   \n",
      "4  29 Syria Rebels Dead in Fighting for Key Alepp...   \n",
      "\n",
      "                                                text label  \n",
      "0  Syria attack symptoms consistent with nerve ag...  FAKE  \n",
      "1  at 0914 Homs governor says U.S. attack caused ...  FAKE  \n",
      "2  Death toll from Aleppo bomb attack at least 11...  FAKE  \n",
      "3  Aleppo bomb blast kills six Syrian state TV. A...  FAKE  \n",
      "4  29 Syria Rebels Dead in Fighting for Key Alepp...  FAKE  \n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    leer_csv(set)\n",
    "    print(leer_csv(set).head()) #print the first 5 rows of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6306, 3)\n",
      "(3522, 3)\n",
      "(39105, 3)\n",
      "(3522, 3)\n",
      "(799, 3)\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    print(leer_csv(set).shape) #print the shape of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    print(leer_csv(set).duplicated().sum()) #print the number of duplicated rows in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n",
      "Index(['title', 'text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    print(leer_csv(set).columns) #print the columns of each dataset, enusre that the columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in dataset_1.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_3.csv:\n",
      "['REAL' 'FAKE']\n",
      "Unique values in dataset_4.csv:\n",
      "['FAKE' 'REAL']\n",
      "Unique values in dataset_5.csv:\n",
      "['REAL' 'FAKE']\n",
      "Unique values in dataset_6.csv:\n",
      "['FAKE' 'REAL']\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Unique values in {set}:\")\n",
    "    print(df['label'].unique()) #print the unique values in the label column of each dataset, ensure uniformity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in dataset_1.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_3.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_4.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_5.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_6.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set in datasets:\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Null values in {set}:\")\n",
    "    print(df.isnull().sum()) #print the number of null values in each dataset, one value detected in dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset_3 = leer_csv('dataset_3.csv') #read dataset_3, drop the null value and save the cleaned dataset\n",
    "df_dataset_3= df_dataset_3.dropna()\n",
    "df_dataset_3.to_csv('dataset_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in dataset_1.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_3.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_4.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_5.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n",
      "Null values in dataset_6.csv:\n",
      "title    0\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for set in datasets: #confirm no remaining null values\n",
    "    df = leer_csv(set)\n",
    "    print(f\"Null values in {set}:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53254, 3)\n"
     ]
    }
   ],
   "source": [
    "df_list = []  # Initialize an empty list to store DataFrames\n",
    "\n",
    "for dataset in datasets:  # Loop through each file name\n",
    "    df_list.append(leer_csv(dataset))  # Append the DataFrame to the list\n",
    "\n",
    "df_combined = pd.concat(df_list, ignore_index=True)  # Merge all DataFrames\n",
    "\n",
    "print(df_combined.shape)  # Check the size of the final dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Daniel Greenfield, a Shillman Journalism Fellow at the Freedom Center, is a New York writer focusing on radical Islam. \\nIn the final stretch of the election, Hillary Rodham Clinton has gone to war with the FBI. \\nThe word “unprecedented” has been thrown around so often this election that it ought to be retired. But it’s still unprecedented for the nominee of a major political party to go war with the FBI. \\nBut that’s exactly what Hillary and her people have done. Coma patients just waking up now and watching an hour of CNN from their hospital beds would assume that FBI Director James Comey is Hillary’s opponent in this election. \\nThe FBI is under attack by everyone from Obama to CNN. Hillary’s people have circulated a letter attacking Comey. There are currently more media hit pieces lambasting him than targeting Trump. It wouldn’t be too surprising if the Clintons or their allies were to start running attack ads against the FBI. \\nThe FBI’s leadership is being warned that the entire left-wing establishment will form a lynch mob if they continue going after Hillary. And the FBI’s credibility is being attacked by the media and the Democrats to preemptively head off the results of the investigation of the Clinton Foundation and Hillary Clinton. \\nThe covert struggle between FBI agents and Obama’s DOJ people has gone explosively public. \\nThe New York Times has compared Comey to J. Edgar Hoover. Its bizarre headline, “James Comey Role Recalls Hoover’s FBI, Fairly or Not” practically admits up front that it’s spouting nonsense. The Boston Globe has published a column calling for Comey’s resignation. Not to be outdone, Time has an editorial claiming that the scandal is really an attack on all women. \\nJames Carville appeared on MSNBC to remind everyone that he was still alive and insane. He accused Comey of coordinating with House Republicans and the KGB. And you thought the “vast right wing conspiracy” was a stretch. \\nCountless media stories charge Comey with violating procedure. Do you know what’s a procedural violation? Emailing classified information stored on your bathroom server. \\nSenator Harry Reid has sent Comey a letter accusing him of violating the Hatch Act. The Hatch Act is a nice idea that has as much relevance in the age of Obama as the Tenth Amendment. But the cable news spectrum quickly filled with media hacks glancing at the Wikipedia article on the Hatch Act under the table while accusing the FBI director of one of the most awkward conspiracies against Hillary ever. \\nIf James Comey is really out to hurt Hillary, he picked one hell of a strange way to do it. \\nNot too long ago Democrats were breathing a sigh of relief when he gave Hillary Clinton a pass in a prominent public statement. If he really were out to elect Trump by keeping the email scandal going, why did he trash the investigation? Was he on the payroll of House Republicans and the KGB back then and playing it coy or was it a sudden development where Vladimir Putin and Paul Ryan talked him into taking a look at Anthony Weiner’s computer? \\nEither Comey is the most cunning FBI director that ever lived or he’s just awkwardly trying to navigate a political mess that has trapped him between a DOJ leadership whose political futures are tied to Hillary’s victory and his own bureau whose apolitical agents just want to be allowed to do their jobs. \\nThe only truly mysterious thing is why Hillary and her associates decided to go to war with a respected Federal agency. Most Americans like the FBI while Hillary Clinton enjoys a 60% unfavorable rating. \\nAnd it’s an interesting question. \\nHillary’s old strategy was to lie and deny that the FBI even had a criminal investigation underway. Instead her associates insisted that it was a security review. The FBI corrected her and she shrugged it off. But the old breezy denial approach has given way to a savage assault on the FBI. \\nPretending that nothing was wrong was a bad strategy, but it was a better one that picking a fight with the FBI while lunatic Clinton associates try to claim that the FBI is really the KGB. \\nThere are two possible explanations. \\nHillary Clinton might be arrogant enough to lash out at the FBI now that she believes that victory is near. The same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the FBI for irritating her during the final miles of her campaign. \\nBut the other explanation is that her people panicked. \\nGoing to war with the FBI is not the behavior of a smart and focused presidential campaign. It’s an act of desperation. When a presidential candidate decides that her only option is to try and destroy the credibility of the FBI, that’s not hubris, it’s fear of what the FBI might be about to reveal about her. \\nDuring the original FBI investigation, Hillary Clinton was confident that she could ride it out. And she had good reason for believing that. But that Hillary Clinton is gone. In her place is a paranoid wreck. Within a short space of time the “positive” Clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the FBI. \\nThere’s only one reason for such bizarre behavior. \\nThe Clinton campaign has decided that an FBI investigation of the latest batch of emails poses a threat to its survival. And so it’s gone all in on fighting the FBI. It’s an unprecedented step born of fear. It’s hard to know whether that fear is justified. But the existence of that fear already tells us a whole lot. \\nClinton loyalists rigged the old investigation. They knew the outcome ahead of time as well as they knew the debate questions. Now suddenly they are no longer in control. And they are afraid. \\nYou can smell the fear. \\nThe FBI has wiretaps from the investigation of the Clinton Foundation. It’s finding new emails all the time. And Clintonworld panicked. The spinmeisters of Clintonworld have claimed that the email scandal is just so much smoke without fire. All that’s here is the appearance of impropriety without any of the substance. But this isn’t how you react to smoke. It’s how you respond to a fire. \\nThe misguided assault on the FBI tells us that Hillary Clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup. The email setup was a preemptive cover up. The Clinton campaign has panicked badly out of the belief, right or wrong, that whatever crime the illegal setup was meant to cover up is at risk of being exposed. \\nThe Clintons have weathered countless scandals over the years. Whatever they are protecting this time around is bigger than the usual corruption, bribery, sexual assaults and abuses of power that have followed them around throughout the years. This is bigger and more damaging than any of the allegations that have already come out. And they don’t want FBI investigators anywhere near it. \\nThe campaign against Comey is pure intimidation. It’s also a warning. Any senior FBI people who value their careers are being warned to stay away. The Democrats are closing ranks around their nominee against the FBI. It’s an ugly and unprecedented scene. It may also be their last stand. \\nHillary Clinton has awkwardly wound her way through numerous scandals in just this election cycle. But she’s never shown fear or desperation before. Now that has changed. Whatever she is afraid of, it lies buried in her emails with Huma Abedin. And it can bring her down like nothing else has.  ',\n",
       "       'Google Pinterest Digg Linkedin Reddit Stumbleupon Print Delicious Pocket Tumblr \\nThere are two fundamental truths in this world: Paul Ryan desperately wants to be president. And Paul Ryan will never be president. Today proved it. \\nIn a particularly staggering example of political cowardice, Paul Ryan re-re-re-reversed course and announced that he was back on the Trump Train after all. This was an aboutface from where he was a few weeks ago. He had previously declared he would not be supporting or defending Trump after a tape was made public in which Trump bragged about assaulting women. Suddenly, Ryan was appearing at a pro-Trump rally and boldly declaring that he already sent in his vote to make him President of the United States. It was a surreal moment. The figurehead of the Republican Party dosed himself in gasoline, got up on a stage on a chilly afternoon in Wisconsin, and lit a match. . @SpeakerRyan says he voted for @realDonaldTrump : “Republicans, it is time to come home” https://t.co/VyTT49YvoE pic.twitter.com/wCvSCg4a5I \\n— ABC News Politics (@ABCPolitics) November 5, 2016 \\nThe Democratic Party couldn’t have asked for a better moment of film. Ryan’s chances of ever becoming president went down to zero in an instant. In the wreckage Trump is to leave behind in his wake, those who cravenly backed his campaign will not recover. If Ryan’s career manages to limp all the way to 2020, then the DNC will have this tape locked and loaded to be used in every ad until Election Day. \\nThe ringing endorsement of the man he clearly hates on a personal level speaks volumes about his own spinelessness. Ryan has postured himself as a “principled” conservative, and one uncomfortable with Trump’s unapologetic bigotry and sexism. However, when push came to shove, Paul Ryan – like many of his colleagues – turned into a sniveling appeaser. After all his lofty tak about conviction, his principles were a house of cards and collapsed with the slightest breeze. \\nWhat’s especially bizarre is how close Ryan came to making it through unscathed. For months the Speaker of the House refused to comment on Trump at all. His strategy seemed to be to keep his head down, pretend Trump didn’t exist, and hope that nobody remembered what happened in 2016. Now, just days away from the election, he screwed it all up. \\nIf 2016’s very ugly election has done any good it’s by exposing the utter cowardice of the Republicans who once feigned moral courage. A reality television star spit on them, hijacked their party, insulted their wives, and got every last one of them to kneel before him. What a turn of events. \\nFeatured image via Twitter',\n",
       "       'U.S. Secretary of State John F. Kerry said Monday that he will stop in Paris later this week, amid criticism that no top American officials attended Sunday’s unity march against terrorism.\\n\\nKerry said he expects to arrive in Paris Thursday evening, as he heads home after a week abroad. He said he will fly to France at the conclusion of a series of meetings scheduled for Thursday in Sofia, Bulgaria. He plans to meet the next day with Foreign Minister Laurent Fabius and President Francois Hollande, then return to Washington.\\n\\nThe visit by Kerry, who has family and childhood ties to the country and speaks fluent French, could address some of the criticism that the United States snubbed France in its darkest hour in many years.\\n\\nThe French press on Monday was filled with questions about why neither President Obama nor Kerry attended Sunday’s march, as about 40 leaders of other nations did. Obama was said to have stayed away because his own security needs can be taxing on a country, and Kerry had prior commitments.\\n\\nAmong roughly 40 leaders who did attend was Israeli Prime Minister Benjamin Netanyahu, no stranger to intense security, who marched beside Hollande through the city streets. The highest ranking U.S. officials attending the march were Jane Hartley, the ambassador to France, and Victoria Nuland, the assistant secretary of state for European affairs. Attorney General Eric H. Holder Jr. was in Paris for meetings with law enforcement officials but did not participate in the march.\\n\\nKerry spent Sunday at a business summit hosted by India’s prime minister, Narendra Modi. The United States is eager for India to relax stringent laws that function as barriers to foreign investment and hopes Modi’s government will act to open the huge Indian market for more American businesses.\\n\\nIn a news conference, Kerry brushed aside criticism that the United States had not sent a more senior official to Paris as “quibbling a little bit.” He noted that many staffers of the American Embassy in Paris attended the march, including the ambassador. He said he had wanted to be present at the march himself but could not because of his prior commitments in India.\\n\\n“But that is why I am going there on the way home, to make it crystal clear how passionately we feel about the events that have taken place there,” he said.\\n\\n“And I don’t think the people of France have any doubts about America’s understanding of what happened, of our personal sense of loss and our deep commitment to the people of France in this moment of trauma.”',\n",
       "       ...,\n",
       "       '03-08-2016 Chemical Attack Kills Five Syrians in Aleppo SANA. At least five Syrians have been killed and a number of others injured in a chemical attack by foreign-sponsored Takfiri militants against a residential neighborhood in northwestern Syria At least five Syrians have been killed and a number of others injured in a chemical attack by foreign-sponsored Takfiri militants against a residential neighborhood in northwestern Syria. Health director for Aleppo Mohammad Hazouri said five people died and eight others experienced breathing difficulties after artillery shells containing toxic gasses slammed into the Old City of Aleppo on Tuesday the official SANA news agency reported. Government sources said Takfiri terrorists had also used chemical munitions against civilians in the city of Saraqib in the Idlib province but militants accused government forces of carrying out the attack. Doctor Ibrahim al-Assad a neurologist in Saraqib said he treated 16 of 29 cases brought to his hospital on Monday night. He added that most of the victims were women and children and were suffering from breathing difficulties red eyes and wheezing. Rescuers and doctors in the city said the symptoms were similar to those caused by chlorine gas. The chemical raids come as the Syrian army is making progress in operations to retake Aleppo from militants who are seeing the noose tightening around them in the areas which they control. ',\n",
       "       '01-08-2016 5 Killed as Russian Military Chopper Shot down in Syria. All five people on board a Russian military helicopter that was shot down over Syria on Monday are believed to have died the Kremlin said. All five people on board a Russian military helicopter that was shot down over Syria on Monday are believed to have died the Kremlin said. The Russian defense ministry said the Russian military helicopter was shot down over the Syrian province of Idlib. \"A Russian Mi-8 military transport helicopter was shot down from the ground in Idlib province after delivering humanitarian aid to Aleppo\" the ministry said in a statement. \"There were three crew members and two officers from the Russian reconciliation centre in Syria on board\" it said. Kremlin spokesman Dmitry Peskov expressed condolences over the deaths of the five soldiers. \"As far as we know from the information weve had from the defense ministry those in the helicopter died they died heroically because they were trying to move the aircraft away to minimize victims on the ground\" he told journalists. Photos shared widely on social networks by Syrian opposition on Monday purported to depict the smoking craft in the desert and personal belongings of those inside including Russian drivers licenses passports and insurance cards as well as Orthodox Christian icons. The authenticity of the pictures could not be independently confirmed. The Monday downing brought the toll for Russian soldiers killed in the Syrian conflict to 18. ',\n",
       "       'April 6 2017 Syrian Army Kills 48 ISIL Terrorists in Deir Ezzor. A military source announced that control was established over Telal Hakema ( Hakema hills) in the western direction of Deir Ezzor after destroying the fortifications of ISIL terrorists in it. The source told SANA that army units in cooperation with the backing forces over the past few hours carried out special operations against gatherings of ISISL terrorists in the western direction of Deir Ezzor establishing control over the hills of Hilal al-Dusham (the barricades) and Milad after destroying the last ISIS gatherings in them. The source added that army units carried out intensive bombardments in the surrounding of al-Maqaber cemeteries area al-Masane (the factories) and the water plant on the southern outskirts of Deir Ezzor killing 48 terrorists and destroying a tank and 6 machinegun-equipped vehicles. Lattakia Army units operating in Lattakia countryside thwarted an attack launched by al-Nusra Front terrorists on the axes of KabaniKensabba in the northeastern countryside of Lattakia province. SANA reporter in Lattakia said the army units in cooperation with the supporting forces destroyed 3 vehicles rigged with explosives for terrorist organizations before they could reach the area surrounding a number of military points and safety villages on the axes of KabaniKensabba. The reporter added that army units and terrorist groups engaged in heavy clashes when terrorists tried to advance toward the safety areas after the destruction of the vehicles. A large number of terrorists most of them are members of al-Nusra were killed before others fled away towards areas near the Turkish-Syrian border. Daraa Army units killed a number of al-Nusra terrorists in Daraa al-Balad and al-Lajat areas and foiled their attacks on one of the military posts in the northwestern countryside. A military source told SANA that an army unit foiled an attempt by al-Nusra terrorist groups to infiltrate to one of the military posts in Jadieh village 65 km northwest of Daraa city after killing most of the terrorists while the others fled away. The source added that an army unit killed in special operation all members of the terrorist group which was moving from al-Lajat area toward the northern countryside of Sweida. In Daraa al-Balad the military source said that the army carried out intensive strikes against gatherings and movements of al-Nusra terrorists in al-Karak neighborhood and on the left of al-Jomrok al-Qadeem (Old Custom) road killing 8 terrorists and destroying two cannons and two heavy machineguns. Source SANA'],\n",
       "      shape=(48357,), dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(columna):\n",
    "    \"\"\"Function to clean text in the given column of df_combined.\"\"\"\n",
    "    \n",
    "    # Convert to string first (to avoid NaN issues)\n",
    "    df_combined[columna] = df_combined[columna].astype(str)\n",
    "\n",
    "    # Remove dates like 01-01-2020\n",
    "    pattern_date = r\"\\b\\d{2}-\\d{2}-\\d{4}\\b\"\n",
    "    df_combined[columna] = df_combined[columna].str.replace(pattern_date, \"\", regex=True).str.strip()\n",
    "\n",
    "    # Remove dates like January 01 2020 (or Jan 01 2020)\n",
    "    pattern_date2 = r\"\\b(?:jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\\s\\d{1,2}\\s\\d{4}\\b\"\n",
    "    df_combined[columna] = df_combined[columna].str.replace(pattern_date2, \"\", regex=True).str.strip()\n",
    "\n",
    "    # Remove location prefixes like \"Newark, N.J. (Reuters) -\", \"Paris (Reuters) -\", etc.\n",
    "    pattern_location = r\"[a-z\\s]+(?:,\\s?[a-z.\\s]+)?\\s\\([a-z\\s]+\\)\\s?-\\s?\"\n",
    "    df_combined[columna] = df_combined[columna].str.replace(pattern_location, \"\", regex=True).str.strip()\n",
    "\n",
    "    # Expand contractions (e.g., \"can't\" -> \"cannot\")\n",
    "    df_combined[columna] = df_combined[columna].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "    # Remove stopwords\n",
    "    df_combined[columna] = df_combined[columna].apply(lambda x: \" \".join([word for word in x.split() if word.lower() not in stops]))\n",
    "\n",
    "    # Convert text to lowercase at the end\n",
    "    df_combined[columna] = df_combined[columna].str.lower()\n",
    "\n",
    "    # Restore \"U.S.\"\n",
    "    df_combined[columna] = df_combined[columna].str.replace(\"you.s\", \"u.s\", regex=True)\n",
    "\n",
    "    return df_combined\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_clean = ['text', 'title']  # Define the columns to consider\n",
    "\n",
    "for column in columns_to_clean:  # Loop through each column\n",
    "    clean_text(column)  # Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r\"^[a-z\\s]+(?:,\\s?[a-z.\\s]+)?\\s\\([a-z\\s]+\\)\\s?-\\s?\"\n",
    "# Apply regex to clean the text column\n",
    "df_combined['text'] = df_combined['text'].apply(lambda x: re.sub(pattern, \"\", x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29380</th>\n",
       "      <td>republican leader plans senate vote healthcare...</td>\n",
       "      <td>senate republican leader mitch mcconnell plans...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45170</th>\n",
       "      <td>fdp's push invite putin g7 sows discord within...</td>\n",
       "      <td>leader germany pro-business free democrats (fd...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29321</th>\n",
       "      <td>china says ban petroleum exports north korea</td>\n",
       "      <td>china said saturday ban exports petroleum prod...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>lgbt volunteers waiting thrown rooftop…join fi...</td>\n",
       "      <td>group volunteer soldiers announced week first ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12409</th>\n",
       "      <td>look japanese leader’s face meeting trump says...</td>\n",
       "      <td>alleged president donald trump met japanese pr...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22817</th>\n",
       "      <td>scoundrel hillary supporter starts “trumpleaks...</td>\n",
       "      <td>hillary clinton ally david brock offering pay ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26437</th>\n",
       "      <td>kellyanne conway delivers knock punch smug jak...</td>\n",
       "      <td>audience member asks kellyanne conway trump co...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20779</th>\n",
       "      <td>breaking: french vote mass muslim immigration ...</td>\n",
       "      <td>french citizens voted demise elected macron na...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17812</th>\n",
       "      <td>cruz leaves closet disappointed secret meeting...</td>\n",
       "      <td>gets creepier creepier ted cruz. time, took be...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>left turns bob dylan pro-israel views, refusal...</td>\n",
       "      <td>left turns bob dylan pro-israel views, refusal...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>11 gop senators tweeted support mcconnell…prim...</td>\n",
       "      <td>arrogant smug call mcconnell spoke group telli...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11180</th>\n",
       "      <td>ben carson believes poverty ‘state mind’ peopl...</td>\n",
       "      <td>ben carson, head department housing urban deve...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>chris christie kicks comeback tour n.h., going...</td>\n",
       "      <td>goffstown – chris christie kicked two day swin...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41088</th>\n",
       "      <td>u.s. urges authorities review honduras electio...</td>\n",
       "      <td>united states urged election authorities revie...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>exclusive: siege islamabad</td>\n",
       "      <td>brig asif h. raja october 31, 2016 asif haroon...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "29380  republican leader plans senate vote healthcare...   \n",
       "45170  fdp's push invite putin g7 sows discord within...   \n",
       "29321       china says ban petroleum exports north korea   \n",
       "20143  lgbt volunteers waiting thrown rooftop…join fi...   \n",
       "12409  look japanese leader’s face meeting trump says...   \n",
       "22817  scoundrel hillary supporter starts “trumpleaks...   \n",
       "26437  kellyanne conway delivers knock punch smug jak...   \n",
       "20779  breaking: french vote mass muslim immigration ...   \n",
       "17812  cruz leaves closet disappointed secret meeting...   \n",
       "713    left turns bob dylan pro-israel views, refusal...   \n",
       "19997  11 gop senators tweeted support mcconnell…prim...   \n",
       "11180  ben carson believes poverty ‘state mind’ peopl...   \n",
       "2408   chris christie kicks comeback tour n.h., going...   \n",
       "41088  u.s. urges authorities review honduras electio...   \n",
       "270                           exclusive: siege islamabad   \n",
       "\n",
       "                                                    text label  \n",
       "29380  senate republican leader mitch mcconnell plans...  REAL  \n",
       "45170  leader germany pro-business free democrats (fd...  REAL  \n",
       "29321  china said saturday ban exports petroleum prod...  REAL  \n",
       "20143  group volunteer soldiers announced week first ...  FAKE  \n",
       "12409  alleged president donald trump met japanese pr...  FAKE  \n",
       "22817  hillary clinton ally david brock offering pay ...  FAKE  \n",
       "26437  audience member asks kellyanne conway trump co...  FAKE  \n",
       "20779  french citizens voted demise elected macron na...  FAKE  \n",
       "17812  gets creepier creepier ted cruz. time, took be...  FAKE  \n",
       "713    left turns bob dylan pro-israel views, refusal...  FAKE  \n",
       "19997  arrogant smug call mcconnell spoke group telli...  FAKE  \n",
       "11180  ben carson, head department housing urban deve...  FAKE  \n",
       "2408   goffstown – chris christie kicked two day swin...  REAL  \n",
       "41088  united states urged election authorities revie...  REAL  \n",
       "270    brig asif h. raja october 31, 2016 asif haroon...  FAKE  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['smell hillary’s fear',\n",
       "       'watch exact moment paul ryan committed political suicide trump rally (video)',\n",
       "       'kerry go paris gesture sympathy', ...,\n",
       "       'chemical attack kills five syrians aleppo sana',\n",
       "       '5 killed russian military chopper shot syria',\n",
       "       'syrian army kills 48 isil terrorists deir ezzor'],\n",
       "      shape=(48339,), dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test datasets\n",
    "train_df, test_df = train_test_split(df_combined, test_size=0.2, random_state=42) #test size .02 indicates a 80/20 split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (42603, 3), test shape: (10651, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'train shape: {train_df.shape}, test shape: {test_df.shape}')  # Check the size of the final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_data.csv\")\n",
    "test_df = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title      0\n",
       "text     383\n",
       "label      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with null values\n",
    "train_df = train_df.dropna(subset=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title               0\n",
       "text              100\n",
       "label               0\n",
       "tokenized_text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title    0\n",
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization: word_tokenize function from the nltk.tokenize module to break the input text into individual words (tokens). For example, \"I love coding!\" would become ['I', 'love', 'coding', '!'].\n",
    "\n",
    "Lemmatization: reduces a word to its base or root form. For instance, \"running\" would become \"run\" and \"better\" would become \"good\". Uses WordNetLemmatizer from the nltk.stem module. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        tokens = word_tokenize(text)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        # Lemmatize while keeping only alphabetic words using list comprehension\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "        return tokens\n",
    "    else:\n",
    "        return []  # Return an empty list if the input is not a string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Kathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['tokenized_text'] = test_df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>puerto rican police officer: san juan mayor al...</td>\n",
       "      <td>puerto rican police officer: san juan mayor al...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[puerto, rican, police, officer, san, juan, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brazil's president recovering prostate surgery</td>\n",
       "      <td>brazilian president michel temer recovering su...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[brazilian, president, michel, temer, recoveri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white house facing rocky legal road immigration</td>\n",
       "      <td>campaign, trump threatened impose large tariff...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[campaign, trump, threatened, impose, large, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch: newt gingrich admits trump care unity, ...</td>\n",
       "      <td>democrats offering work donald trump cease des...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[democrat, offering, work, donald, trump, ceas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump outlines treat america like bankrupt bus...</td>\n",
       "      <td>furniture company southern maine called chapte...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[furniture, company, southern, maine, called, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  puerto rican police officer: san juan mayor al...   \n",
       "1     brazil's president recovering prostate surgery   \n",
       "2    white house facing rocky legal road immigration   \n",
       "3  watch: newt gingrich admits trump care unity, ...   \n",
       "4  trump outlines treat america like bankrupt bus...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  puerto rican police officer: san juan mayor al...  FAKE   \n",
       "1  brazilian president michel temer recovering su...  REAL   \n",
       "2  campaign, trump threatened impose large tariff...  REAL   \n",
       "3  democrats offering work donald trump cease des...  FAKE   \n",
       "4  furniture company southern maine called chapte...  FAKE   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [puerto, rican, police, officer, san, juan, ma...  \n",
       "1  [brazilian, president, michel, temer, recoveri...  \n",
       "2  [campaign, trump, threatened, impose, large, t...  \n",
       "3  [democrat, offering, work, donald, trump, ceas...  \n",
       "4  [furniture, company, southern, maine, called, ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bet hip hop awards 2017: winners list</td>\n",
       "      <td>story highlights cardi b. performed also singl...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[story, highlight, cardi, performed, also, sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hippie throwback: video photos emerge socialis...</td>\n",
       "      <td>pathetic little basement dweller original occu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[pathetic, little, basement, dweller, original...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>louisiana’s dem. governor steps up, signs exec...</td>\n",
       "      <td>could anyone, million years, ever see bobby ji...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[could, anyone, million, year, ever, see, bobb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>police: kalamazoo spree killer uber driver, pi...</td>\n",
       "      <td>disturbing twist already-terrifying story, man...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[disturbing, twist, story, man, believed, behi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[video] leftist cnn host actually said this: c...</td>\n",
       "      <td>shamelessly bashing people think human action ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[shamelessly, bashing, people, think, human, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              bet hip hop awards 2017: winners list   \n",
       "1  hippie throwback: video photos emerge socialis...   \n",
       "2  louisiana’s dem. governor steps up, signs exec...   \n",
       "3  police: kalamazoo spree killer uber driver, pi...   \n",
       "4  [video] leftist cnn host actually said this: c...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  story highlights cardi b. performed also singl...  REAL   \n",
       "1  pathetic little basement dweller original occu...  FAKE   \n",
       "2  could anyone, million years, ever see bobby ji...  FAKE   \n",
       "3  disturbing twist already-terrifying story, man...  FAKE   \n",
       "4  shamelessly bashing people think human action ...  FAKE   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  [story, highlight, cardi, performed, also, sin...  \n",
       "1  [pathetic, little, basement, dweller, original...  \n",
       "2  [could, anyone, million, year, ever, see, bobb...  \n",
       "3  [disturbing, twist, story, man, believed, behi...  \n",
       "4  [shamelessly, bashing, people, think, human, a...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df))  # Should output: <class 'pandas.core.frame.DataFrame'>\n",
    "print(type(test_df))   # Should output: <class 'pandas.core.frame.DataFrame'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'text', 'label', 'tokenized_text'], dtype='object')\n",
      "Index(['title', 'text', 'label', 'tokenized_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns)  # Check if 'text' is in the columns of train_df\n",
    "print(test_df.columns)   # Check if 'text' is in the columns of test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_df['text'].isnull().sum())  # Check if there are any NaN values in 'text' column of train_df\n",
    "print(test_df['text'].isnull().sum())   # Check if there are any NaN values in 'text' column of test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized_title'] = train_df['title'].apply(preprocess_text)\n",
    "test_df['tokenized_title'] = test_df['title'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>puerto rican police officer: san juan mayor al...</td>\n",
       "      <td>puerto rican police officer: san juan mayor al...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[puerto, rican, police, officer, san, juan, ma...</td>\n",
       "      <td>[puerto, rican, police, officer, san, juan, ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brazil's president recovering prostate surgery</td>\n",
       "      <td>brazilian president michel temer recovering su...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[brazilian, president, michel, temer, recoveri...</td>\n",
       "      <td>[brazil, president, recovering, prostate, surg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>white house facing rocky legal road immigration</td>\n",
       "      <td>campaign, trump threatened impose large tariff...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[campaign, trump, threatened, impose, large, t...</td>\n",
       "      <td>[white, house, facing, rocky, legal, road, imm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watch: newt gingrich admits trump care unity, ...</td>\n",
       "      <td>democrats offering work donald trump cease des...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[democrat, offering, work, donald, trump, ceas...</td>\n",
       "      <td>[watch, newt, gingrich, admits, trump, care, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trump outlines treat america like bankrupt bus...</td>\n",
       "      <td>furniture company southern maine called chapte...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[furniture, company, southern, maine, called, ...</td>\n",
       "      <td>[trump, outline, treat, america, like, bankrup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  puerto rican police officer: san juan mayor al...   \n",
       "1     brazil's president recovering prostate surgery   \n",
       "2    white house facing rocky legal road immigration   \n",
       "3  watch: newt gingrich admits trump care unity, ...   \n",
       "4  trump outlines treat america like bankrupt bus...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  puerto rican police officer: san juan mayor al...  FAKE   \n",
       "1  brazilian president michel temer recovering su...  REAL   \n",
       "2  campaign, trump threatened impose large tariff...  REAL   \n",
       "3  democrats offering work donald trump cease des...  FAKE   \n",
       "4  furniture company southern maine called chapte...  FAKE   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [puerto, rican, police, officer, san, juan, ma...   \n",
       "1  [brazilian, president, michel, temer, recoveri...   \n",
       "2  [campaign, trump, threatened, impose, large, t...   \n",
       "3  [democrat, offering, work, donald, trump, ceas...   \n",
       "4  [furniture, company, southern, maine, called, ...   \n",
       "\n",
       "                                     tokenized_title  \n",
       "0  [puerto, rican, police, officer, san, juan, ma...  \n",
       "1  [brazil, president, recovering, prostate, surg...  \n",
       "2  [white, house, facing, rocky, legal, road, imm...  \n",
       "3  [watch, newt, gingrich, admits, trump, care, u...  \n",
       "4  [trump, outline, treat, america, like, bankrup...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>tokenized_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bet hip hop awards 2017: winners list</td>\n",
       "      <td>story highlights cardi b. performed also singl...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[story, highlight, cardi, performed, also, sin...</td>\n",
       "      <td>[bet, hip, hop, award, winner, list]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hippie throwback: video photos emerge socialis...</td>\n",
       "      <td>pathetic little basement dweller original occu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[pathetic, little, basement, dweller, original...</td>\n",
       "      <td>[hippie, throwback, video, photo, emerge, soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>louisiana’s dem. governor steps up, signs exec...</td>\n",
       "      <td>could anyone, million years, ever see bobby ji...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[could, anyone, million, year, ever, see, bobb...</td>\n",
       "      <td>[louisiana, s, dem, governor, step, up, sign, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>police: kalamazoo spree killer uber driver, pi...</td>\n",
       "      <td>disturbing twist already-terrifying story, man...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[disturbing, twist, story, man, believed, behi...</td>\n",
       "      <td>[police, kalamazoo, spree, killer, uber, drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[video] leftist cnn host actually said this: c...</td>\n",
       "      <td>shamelessly bashing people think human action ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[shamelessly, bashing, people, think, human, a...</td>\n",
       "      <td>[video, leftist, cnn, host, actually, said, th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0              bet hip hop awards 2017: winners list   \n",
       "1  hippie throwback: video photos emerge socialis...   \n",
       "2  louisiana’s dem. governor steps up, signs exec...   \n",
       "3  police: kalamazoo spree killer uber driver, pi...   \n",
       "4  [video] leftist cnn host actually said this: c...   \n",
       "\n",
       "                                                text label  \\\n",
       "0  story highlights cardi b. performed also singl...  REAL   \n",
       "1  pathetic little basement dweller original occu...  FAKE   \n",
       "2  could anyone, million years, ever see bobby ji...  FAKE   \n",
       "3  disturbing twist already-terrifying story, man...  FAKE   \n",
       "4  shamelessly bashing people think human action ...  FAKE   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [story, highlight, cardi, performed, also, sin...   \n",
       "1  [pathetic, little, basement, dweller, original...   \n",
       "2  [could, anyone, million, year, ever, see, bobb...   \n",
       "3  [disturbing, twist, story, man, believed, behi...   \n",
       "4  [shamelessly, bashing, people, think, human, a...   \n",
       "\n",
       "                                     tokenized_title  \n",
       "0               [bet, hip, hop, award, winner, list]  \n",
       "1  [hippie, throwback, video, photo, emerge, soci...  \n",
       "2  [louisiana, s, dem, governor, step, up, sign, ...  \n",
       "3  [police, kalamazoo, spree, killer, uber, drive...  \n",
       "4  [video, leftist, cnn, host, actually, said, th...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train_data_tokenized.csv\", index=False)\n",
    "test_df.to_csv(\"test_data_tokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency-Inverse Document Frequency): transform text data into numerical vectors that capture the importance of words in relation to the corpus.\n",
    "\n",
    "With max_features=n: The vectorizer will only keep the top n most important words based on their TF-IDF score (i.e., the highest words according to the frequency of term occurrence relative to its rarity in the corpus). This helps reduce the dimensionality of the data, making the model more efficient and often improving generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features as needed\n",
    "\n",
    "# Apply TF-IDF vectorization to the tokenized text (you might want to rejoin the tokens into strings before this step)\n",
    "train_df['text_str'] = train_df['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "test_df['text_str'] = test_df['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_train_text = vectorizer.fit_transform(train_df['text_str'])\n",
    "X_test_text = vectorizer.transform(test_df['text_str'])\n",
    "\n",
    "# Apply TF-IDF vectorization to the tokenized titles (similar approach as for text)\n",
    "train_df['title_str'] = train_df['tokenized_title'].apply(lambda x: ' '.join(x))\n",
    "test_df['title_str'] = test_df['tokenized_title'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "X_train_title = vectorizer.fit_transform(train_df['title_str'])\n",
    "X_test_title = vectorizer.transform(test_df['title_str'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection of Logistic Regression as training model,principally for the simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9704642349597348\n",
      "Test Accuracy: 0.9541275708463652\n"
     ]
    }
   ],
   "source": [
    "#horizontal stack the text and title feature matrices\n",
    "X_train_combined = hstack([X_train_text, X_train_title])\n",
    "X_test_combined = hstack([X_test_text, X_test_title])\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_combined, train_df['label'])\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_combined)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Training Accuracy:\", accuracy_score(train_df['label'], model.predict(X_train_combined)))\n",
    "print(\"Test Accuracy:\", accuracy_score(test_df['label'], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
